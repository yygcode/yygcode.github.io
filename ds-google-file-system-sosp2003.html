<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="zh-CN" xml:lang="zh-CN">
<head>
<!-- 2019-12-22 Sun 21:49 -->
<meta charset="utf-8" />
<title>Google File System</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung, Google" />
<script type="text/javascript"> var disqus_developer = 1; </script>
<link rel="stylesheet" type="text/css" href="styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="styles/readtheorg/css/readtheorg.css"/>
<script src="styles/jquery/2.1.3/jquery.min.js"></script>
<script src="styles/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="styles/readtheorg/js/readtheorg.js"></script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href=""> UP </a>
 |
 <a accesskey="H" href="index.html"> HOME </a>
</div><div id="content">
<h1 class="title">Google File System</h1>
<div id="table-of-contents">
<h2>&#30446;&#24405;</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org1f20f23">1. DECLARATION</a></li>
<li><a href="#org2a505f3">2. Abstract</a></li>
<li><a href="#org4831e42">3. Categories and Subject Descriptors</a></li>
<li><a href="#org93c6eac">4. General Terms</a></li>
<li><a href="#orgeec5baf">5. Keywords</a></li>
<li><a href="#org007fae1">6. Copyright</a></li>
<li><a href="#orge4c43b0">7. INTRODUCTION</a></li>
<li><a href="#orga9ee108">8. DESIGN OVERVIEW</a>
<ul>
<li><a href="#org2a6baea">8.1. Assumptions</a></li>
<li><a href="#orgffcee88">8.2. Interface</a></li>
<li><a href="#orgb6a8ce3">8.3. Architecture</a></li>
<li><a href="#org42694b6">8.4. Single Master</a></li>
<li><a href="#orga1824fe">8.5. Chunk Size</a></li>
<li><a href="#org80cb917">8.6. Metadata</a></li>
<li><a href="#orge41e91c">8.7. Consistency Model</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org1f20f23" class="outline-2">
<h2 id="org1f20f23"><span class="section-number-2">1</span> DECLARATION</h2>
<div class="outline-text-2" id="text-1">
<p>
This page is the paper <a id="org321fb24"></a>. Original Paper Link is:
<a href="https://research.google.com/archive/gfs-sosp2003.pdf">https://research.google.com/archive/gfs-sosp2003.pdf</a>
</p>
</div>
</div>

<div id="outline-container-org2a505f3" class="outline-2">
<h2 id="org2a505f3"><span class="section-number-2">2</span> Abstract</h2>
<div class="outline-text-2" id="text-2">
<p>
We have designed and implemented the Google File System, a scalable distributed
file system for large distributed data-intensive applications. It provides fault
tolerance while running on inexpensive commodity hardware, and it delivers high
aggregate performance to a large number of clients.
</p>

<p>
While sharing many of the same goals as previous distributed file systems, our
design has been driven by observations of our application workloads and
technological environment, both current and anticipated, that reflect a marked
departure from some earlier file system assumptions. This has led us to
reexamine traditional choices and explore radically different design points.
</p>

<p>
The file system has successfully met our storage needs. It is widely deployed
within Google as the storage platform for the generation and processing of data
used by our service as well as research and development efforts that require
large data sets. The largest cluster to date provides hundreds of terabytes of
storage across thousands of disks on over a thousand machines, and it is
concurrently accessed by hundreds of clients.
</p>

<p>
In this paper, we present file system interface extensions designed to support
distributed applications, discuss many aspects of our design, and report
measurements from both micro-benchmarks and real world use.
</p>
</div>
</div>

<div id="outline-container-org4831e42" class="outline-2">
<h2 id="org4831e42"><span class="section-number-2">3</span> Categories and Subject Descriptors</h2>
<div class="outline-text-2" id="text-3">
<p>
D [4]: 3 - Distributed File Systems
</p>
</div>
</div>

<div id="outline-container-org93c6eac" class="outline-2">
<h2 id="org93c6eac"><span class="section-number-2">4</span> General Terms</h2>
<div class="outline-text-2" id="text-4">
<p>
Design, reliability, performance, measurement
</p>
</div>
</div>

<div id="outline-container-orgeec5baf" class="outline-2">
<h2 id="orgeec5baf"><span class="section-number-2">5</span> Keywords</h2>
<div class="outline-text-2" id="text-5">
<p>
Fault tolerance, scalability, data storage, clustered storage
</p>
</div>
</div>

<div id="outline-container-org007fae1" class="outline-2">
<h2 id="org007fae1"><span class="section-number-2">6</span> Copyright</h2>
<div class="outline-text-2" id="text-6">
<p>
The authors can be reached at the following addresses:
{sanjay,hgobioff,shuntak}@google.com.
</p>

<p>
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise,
to republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee.
</p>

<p>
SOSP’03, October 19–22, 2003, Bolton Landing, New York, USA.
Copyright 2003 ACM 1-58113-757-5/03/0010 $5.00.
</p>
</div>
</div>

<div id="outline-container-orge4c43b0" class="outline-2">
<h2 id="orge4c43b0"><span class="section-number-2">7</span> INTRODUCTION</h2>
<div class="outline-text-2" id="text-7">
<p>
We have designed and implemented the Google File System (GFS) to meet the
rapidly growing demands of Google's data processing needs. GFS shares many of
the same goals as previous distributed file systems such as performance,
scalability, reliability, and availability. However, its design has been driven
by key observations of our application workloads and technological environment,
both current and anticipated, that reflect a marked departure from some earlier
file system design assumptions. We have reexamined traditional choices and
explored radically different points in the design space.
</p>

<p>
First, component failures are the norm rather than the exception. The file
system consists of hundreds or even thousands of storage machines built from
inexpensive commodity parts and is accessed by a comparable number of client
machines. The quantity and quality of the components virtually guarantee that
some are not functional at any given time and some will not recover from their
current failures. We have seen problems caused by application bugs, operating
system bugs, human errors, and the failures of disks, memory, connectors,
networking, and power supplies. Therefore, constant monitoring, error detection,
fault tolerance, and automatic recovery must be integral to the system.
</p>

<p>
Second, files are huge by traditional standards. Multi-GB files are common. Each
file typically contains many application objects such as web documents. When we
are regularly working with fast growing data sets of many TBs comprising
billions of objects, it is unwieldy to manage billions of approximately KB-sized
files even when the file system could support it. As a result, design
assumptions and parameters such as I/O operation and block sizes have to be
revisited.
</p>

<p>
Third, most files are mutated by appending new data rather than overwriting
existing data. Random writes within a file are practically non-existent. Once
written, the files are only read, and often only sequentially. A variety of
data share these characteristics. Some may constitute large repositories that
data analysis programs scan through. Some may be data streams continuously
generated by running applications. Some may be archival data. Some may be
intermediate results produced on one machine and processed on another, whether
simultaneously or later in time. Given this access pattern on huge files,
appending becomes the focus of performance optimization and atomicity
guarantees, while caching data blocks in the client loses its appeal.
</p>

<p>
Fourth, co-designing the applications and the file system API benefits the
overall system by increasing our fiexibility. For example, we have relaxed
GFS's consistency model to vastly simplify the file system without imposing
an onerous burden on the applications. We have also introduced an atomic append
operation so that multiple clients can append concurrently to a file without
extra synchronization between them. These will be discussed in more details
later in the paper.
</p>

<p>
Multiple GFS clusters are currently deployed for different purposes. The largest
ones have over 1000 storage nodes, over 300 TB of disk storage, and are heavily
accessed by hundreds of clients on distinct machines on a continuous basis.
</p>
</div>
</div>

<div id="outline-container-orga9ee108" class="outline-2">
<h2 id="orga9ee108"><span class="section-number-2">8</span> DESIGN OVERVIEW</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org2a6baea" class="outline-3">
<h3 id="org2a6baea"><span class="section-number-3">8.1</span> Assumptions</h3>
<div class="outline-text-3" id="text-8-1">
<p>
In designing a file system for our needs, we have been guided by assumptions
that offer both challenges and opportunities. We alluded to some key
observations earlier and now lay out our assumptions in more details.
</p>

<ul class="org-ul">
<li>The system is built from many inexpensive commodity components that often</li>
</ul>
<p>
fail. It must constantly monitor itself and detect, tolerate, and recover
promptly from component failures on a routine basis.
</p>

<ul class="org-ul">
<li>The system stores a modest number of large files. We expect a few million</li>
</ul>
<p>
files, each typically 100 MB or larger in size. Multi-GB files are the common
case and should be managed efficiently. Small files must be supported, but we
need not optimize for them.
</p>

<ul class="org-ul">
<li>The workloads primarily consist of two kinds of reads: large streaming reads</li>
</ul>
<p>
and small random reads. In large streaming reads, individual operations
typically read hundreds of KBs, more commonly 1 MB or more. Successive
operations from the same client often read through a contiguous region of a
file. A small random read typically reads a few KBs at some arbitrary offset.
Performance-conscious applications often batch and sort their small reads to
advance steadily through the file rather than go back and forth.
</p>

<ul class="org-ul">
<li>The workloads also have many large, sequential writes that append data to</li>
</ul>
<p>
files. Typical operation sizes are similar to those for reads. Once written,
files are seldom modified again. Small writes at arbitrary positions in a file
are supported but do not have to be efficient.
</p>

<ul class="org-ul">
<li>The system must efficiently implement well-defined semantics for multiple</li>
</ul>
<p>
clients that concurrently append to the same file. Our files are often used as
producer-consumer queues or for many-way merging. Hundreds of producers, running
one per machine, will concurrently append to a file. Atomicity with minimal
synchronization overhead is essential. The file may be read later, or a consumer
may be reading through the file simultaneously.
</p>

<ul class="org-ul">
<li>High sustained bandwidth is more important than low latency. Most of our</li>
</ul>
<p>
target applications place a premium on processing data in bulk at a high rate,
while few have stringent response time requirements for an individual read or
write.
</p>
</div>
</div>

<div id="outline-container-orgffcee88" class="outline-3">
<h3 id="orgffcee88"><span class="section-number-3">8.2</span> Interface</h3>
<div class="outline-text-3" id="text-8-2">
<p>
GFS provides a familiar file system interface, though it does not implement a
standard API such as POSIX. Files are organized hierarchically in directories
and identified by pathnames. We support the usual operations to create, delete,
open, close, read, and write files.
</p>

<p>
Moreover, GFS has snapshot and record append operations. Snapshot creates a copy
of a file or a directory tree at low cost. Record append allows multiple clients
to append data to the same file concurrently while guaranteeing the atomicity of
each individual client's append. It is useful for implementing multi-way merge
results and producer-consumer queues that many clients can simultaneously append
to without additional locking. We have found these types of files to be
invaluable in building large distributed applications. Snapshot and record
append are discussed further in Sections 3.4 and 3.3 respectively.
</p>
</div>
</div>

<div id="outline-container-orgb6a8ce3" class="outline-3">
<h3 id="orgb6a8ce3"><span class="section-number-3">8.3</span> Architecture</h3>
<div class="outline-text-3" id="text-8-3">
<p>
A GFS cluster consists of a single master and multiple chunkservers and is
accessed by multiple clients, as shown in Figure 1. Each of these is typically a
commodity Linux machine running a user-level server process. It is easy to run
both a chunkserver and a client on the same machine, as long as machine
resources permit and the lower reliability caused by running possibly flaky
application code is acceptable. Files are divided into fixed-size chunks. Each
chunk is identified by an immutable and globally unique 64 bit chunk handle
assigned by the master at the time of chunk creation. Chunkservers store chunks
on local disks as Linux files and read or write chunk data specified by a chunk
handle and byte range. For reliability, each chunk is replicated on multiple
chunkservers. By default, we store three replicas, though users can designate
different replication levels for different regions of the file namespace. The
master maintains all file system metadata. This includes the namespace, access
control information, the map-ping from files to chunks, and the current
locations of chunks. It also controls system-wide activities such as chunk lease
management, garbage collection of orphaned chunks, and chunk migration between
chunkservers. The master periodically communicates with each chunkserver in
HeartBeat messages to give it instructions and collect its state. GFS client
code linked into each application implements the file system API and
communicates with the master and chunkservers to read or write data on behalf of
the application. Clients interact with the master for metadata operations, but
all data-bearing communication goes directly to the chunkservers. We do not
provide the POSIX API and therefore need not hook into the Linux vnode layer.
Neither the client nor the chunkserver caches file data. Client caches offer
little benefit because most applications stream through huge files or have
working sets too large to be cached. Not having them simplifies the client and
the overall system by eliminating cache coherence issues. (Clients do cache
metadata, however.) Chunkservers need not cache file data because chunks are
stored as local files and so Linux's buﬀer cache already keeps frequently
accessed data in memory.
</p>
</div>
</div>

<div id="outline-container-org42694b6" class="outline-3">
<h3 id="org42694b6"><span class="section-number-3">8.4</span> Single Master</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Having a single master vastly simplifies our design and enables the master to
make sophisticated chunk placement and replication decisions using global
knowledge. However, we must minimize its involvement in reads and writes so
that it does not become a bottleneck. Clients never read and write file data
through the master. Instead, a client asks the master which chunkservers it
should contact. It caches this information for a limited time and interacts with
the chunkservers directly for many subsequent operations. Let us explain the
interactions for a simple read with reference to Figure 1. First, using the
fixed chunk size, the client translates the file name and byte offset specified
by the application into a chunk index within the file. Then, it sends the master
a request containing the file name and chunk index. The master replies with the
corresponding chunk handle and locations of the replicas. The client caches this
information using the file name and chunk index as the key. The client then
sends a request to one of the replicas, most likely the closest one. The request
specifies the chunk handle and a byte range within that chunk. Further reads of
the same chunk require no more client-master interaction until the cached
information expires or the file is reopened. In fact, the client typically asks
for multiple chunks in the same request and the master can also include the
information for chunks immediately following those requested. This extra
information sidesteps several future client-master interactions at practically
no extra cost.
</p>
</div>
</div>

<div id="outline-container-orga1824fe" class="outline-3">
<h3 id="orga1824fe"><span class="section-number-3">8.5</span> Chunk Size</h3>
<div class="outline-text-3" id="text-8-5">
<p>
Chunk size is one of the key design parameters. We have chosen 64 MB, which is
much larger than typical file system block sizes. Each chunk replica is stored as
a plain Linux file on a chunkserver and is extended only as needed. Lazy space
allocation avoids wasting space due to internal fragmentation, perhaps the
greatest objection against such a large chunk size.
</p>

<p>
A large chunk size offers several important advantages. First, it reduces
clients' need to interact with the master because reads and writes on the same
chunk require only one initial request to the master for chunk location
information. The reduction is especially significant for our workloads because
applications mostly read and write large files sequentially. Even for small
random reads, the client can comfortably cache all the chunk location
information for a multi-TB working set. Second, since on a large chunk, a client
is more likely to perform many operations on a given chunk, it can reduce
network overhead by keeping a persistent TCP connection to the chunkserver over
an extended period of time. Third, it reduces the size of the metadata stored on
the master. This allows us to keep the metadata in memory, which in turn brings
other advantages that we will discuss in Section 2.6.1.
</p>

<p>
On the other hand, a large chunk size, even with lazy space allocation, has its
disadvantages. A small file consists of a small number of chunks, perhaps just
one. The chunkservers storing those chunks may become hot spots if many clients
are accessing the same file. In practice, hot spots have not been a major issue
because our applications mostly read large multi-chunk files sequentially.
</p>

<p>
However, hot spots did develop when GFS was first used by a batch-queue system:
an executable was written to GFS as a single-chunk file and then started on
hundreds of machines at the same time. The few chunkservers storing this
executable were overloaded by hundreds of simultaneous requests. We fixed this
problem by storing such executables with a higher replication factor and by
making the batch-queue system stagger application start times. A potential
long-term solution is to allow clients to read data from other clients in such
situations.
</p>
</div>
</div>

<div id="outline-container-org80cb917" class="outline-3">
<h3 id="org80cb917"><span class="section-number-3">8.6</span> Metadata</h3>
<div class="outline-text-3" id="text-8-6">
<p>
The master stores three major types of metadata: the file and chunk namespaces,
the mapping from files to chunks, and the locations of each chunk's replicas.
All metadata is kept in the master's memory. The first two types (namespaces and
file-to-chunk mapping) are also kept persistent by logging mutations to an
operation log stored on the master's local disk and replicated on remote
machines. Using a log allows us to update the master state simply, reliably,
and without risking inconsistencies in the event of a master crash. The master
does not store chunk location information persistently. Instead, it asks each
chunkserver about its chunks at master startup and whenever a chunkserver joins
the cluster.
</p>
</div>
<div id="outline-container-org525b597" class="outline-4">
<h4 id="org525b597"><span class="section-number-4">8.6.1</span> In-Memory Data Structures</h4>
<div class="outline-text-4" id="text-8-6-1">
<p>
Since metadata is stored in memory, master operations are fast. Furthermore, it
is easy and efficient for the master to periodically scan through its entire
state in the background. This periodic scanning is used to implement chunk
garbage collection, re-replication in the presence of chunkserver failures, and
chunk migration to balance load and disk space usage across chunkservers.
Sections 4.3 and 4.4 will discuss these activities further.
</p>

<p>
One potential concern for this memory-only approach is that the number of chunks
and hence the capacity of the whole system is limited by how much memory the
master has. This is not a serious limitation in practice. The master maintains
less than 64 bytes of metadata for each 64 MB chunk. Most chunks are full
because most files contain many chunks, only the last of which may be partially
filled. Similarly, the file namespace data typically requires less then 64 bytes
per file because it stores file names compactly using prefix compression.
</p>

<p>
If necessary to support even larger file systems, the cost of adding extra
memory to the master is a small price to pay for the simplicity, reliability,
performance, and flexibility we gain by storing the metadata in memory.
</p>
</div>
</div>

<div id="outline-container-org26dc999" class="outline-4">
<h4 id="org26dc999"><span class="section-number-4">8.6.2</span> Chunk Locations</h4>
<div class="outline-text-4" id="text-8-6-2">
<p>
The master does not keep a persistent record of which chunkservers have a
replica of a given chunk. It simply polls chunkservers for that information
at startup. The master can keep itself up-to-date thereafter because it controls
all chunk placement and monitors chunkserver status with regular HeartBeat
messages.
</p>

<p>
We initially attempted to keep chunk location information persistently at the
master, but we decided that it was much simpler to request the data from
chunkservers at startup, and periodically thereafter. This eliminated the
problem of keeping the master and chunkservers in sync as chunkservers join and
leave the cluster, change names, fail, restart, and so on. In a cluster with
hundreds of servers, these events happen all too often.
</p>

<p>
Another way to understand this design decision is to realize that a chunkserver
has the final word over what chunks it does or does not have on its own disks.
There is no point in trying to maintain a consistent view of this information
on the master because errors on a chunkserver may cause chunks to vanish
spontaneously (e.g., a disk may go bad and be disabled) or an operator may
rename a chunkserver.
</p>
</div>
</div>

<div id="outline-container-org145a212" class="outline-4">
<h4 id="org145a212"><span class="section-number-4">8.6.3</span> Operation Log</h4>
<div class="outline-text-4" id="text-8-6-3">
<p>
The operation log contains a historical record of critical metadata changes. It
is central to GFS. Not only is it the only persistent record of metadata, but it
also serves as a logical time line that defines the order of concurrent
operations. Files and chunks, as well as their versions (see Section 4.5), are
all uniquely and eternally identified by the logical times at which they were
created.
</p>

<p>
Since the operation log is critical, we must store it reliably and not make
changes visible to clients until metadata changes are made persistent.
Otherwise, we effectively lose the whole file system or recent client operations
even if the chunks themselves survive. Therefore, we replicate it on multiple
remote machines and respond to a client operation only after flushing the
corresponding log record to disk both locally and remotely. The master batches
several log records together before flushing thereby reducing the impact of
flushing and replication on overall system throughput.
</p>

<p>
The master recovers its file system state by replaying the operation log. To
minimize startup time, we must keep the log small. The master checkpoints its
state whenever the log grows beyond a certain size so that it can recover by
loading the latest checkpoint from local disk and replaying only the limited
number of log records after that. The checkpoint is in a compact B-tree like
form that can be directly mapped into memory and used for namespace lookup
without extra parsing. This further speeds up recovery and improves
availability.
</p>

<p>
Table 1: File Region State After Mutation
</p>

<p>
Because building a checkpoint can take a while, the master's internal state is
structured in such a way that a new checkpoint can be created without delaying
incoming mutations. The master switches to a new log file and creates the new
checkpoint in a separate thread. The new checkpoint includes all mutations
before the switch. It can be created in a minute or so for a cluster with a few
million files. When completed, it is written to disk both locally and remotely.
</p>

<p>
Recovery needs only the latest complete checkpoint and subsequent log files.
Older checkpoints and log files can be freely deleted, though we keep a few
around to guard against catastrophes. A failure during checkpointing does
not affect correctness because the recovery code detects and skips incomplete
checkpoints.
</p>
</div>
</div>
</div>

<div id="outline-container-orge41e91c" class="outline-3">
<h3 id="orge41e91c"><span class="section-number-3">8.7</span> Consistency Model</h3>
<div class="outline-text-3" id="text-8-7">
<p>
GFS has a relaxed consistency model that supports our highly distributed
applications well but remains relatively simple and efficient to implement. We
now discuss GFS’s guarantees and what they mean to applications. We also
highlight how GFS maintains these guarantees but leave the details to other
parts of the paper.
</p>
</div>

<div id="outline-container-org943bee2" class="outline-4">
<h4 id="org943bee2"><span class="section-number-4">8.7.1</span> Guarantees by GFS</h4>
<div class="outline-text-4" id="text-8-7-1">
<p>
File namespace mutations (e.g., file creation) are atomic. They are handled
exclusively by the master: namespace locking guarantees atomicity and
correctness (Section 4.1); the master’s operation log defines a global total
order of these operations (Section 2.6.3).
</p>

<p>
The state of a file region after a data mutation depends on the type of
mutation, whether it succeeds or fails, and whether there are concurrent
mutations. Table 1 summarizes the result. A file region is consistent if all
clients will always see the same data, regardless of which replicas they read
from. A region is defined after a file data mutation if it is consistent and
clients will see what the mutation writes in its entirety. When a mutation
succeeds without interference from concurrent writers, the affected region is
defined (and by implication consistent): all clients will always see what the
mutation has written. Concurrent successful mutations leave the region undefined
but consistent: all clients see the same data, but it may not reflect what any
one mutation has written. Typically, it consists of mingled fragments from
multiple mutations. A failed mutation makes the region inconsistent (hence also
undefined): different clients may see different data at different times. We
describe below how our applications can distinguish defined regions from
undefined regions. The applications do not need to further distinguish between
different kinds of undefined regions.
</p>

<p>
Data mutations may be writes or record appends. A write causes data to be
written at an application-specified file offset. A record append causes data
(the “record”) to be appended atomically at least once even in the presence of
concurrent mutations, but at an offset of GFS’s choosing (Section 3.3). (In
contrast, a “regular” append is merely a write at an offset that the client
believes to be the current end of file.) The offset is returned to the client
and marks the beginning of a defined region that contains the record. In
addition, GFS may insert padding or record duplicates in between. They occupy
regions considered to be inconsistent and are typically dwarfed by the amount of
user data.
</p>

<p>
After a sequence of successful mutations, the mutated file region is guaranteed
to be defined and contain the data written by the last mutation. GFS achieves
this by (a) applying mutations to a chunk in the same order on all its replicas
(Section 3.1), and (b) using chunk version numbers to detect any replica that
has become stale because it has missed mutations while its chunkserver was down
(Section 4.5). Stale replicas will never be involved in a mutation or given to
clients asking the master for chunk locations. They are garbage collected at the
earliest opportunity.
</p>

<p>
Since clients cache chunk locations, they may read from a stale replica before
that information is refreshed. This window is limited by the cache entry’s
timeout and the next open of the file, which purges from the cache all chunk
information for that file. Moreover, as most of our files are append-only, a
stale replica usually returns a premature end of chunk rather than outdated
data. When a reader retries and contacts the master, it will immediately get
current chunk locations.
</p>

<p>
Long after a successful mutation, component failures can of course still corrupt
or destroy data. GFS identifies failed chunkservers by regular handshakes
between master and all chunkservers and detects data corruption by checksumming
(Section 5.2). Once a problem surfaces, the data is restored from valid replicas
as soon as possible (Section 4.3). A chunk is lost irreversibly only if all its
replicas are lost before GFS can react, typically within minutes. Even in this
case, it becomes unavailable, not corrupted: applications receive clear errors
rather than corrupt data.
</p>
</div>
</div>

<div id="outline-container-orga28201a" class="outline-4">
<h4 id="orga28201a"><span class="section-number-4">8.7.2</span> Implications for Applications</h4>
<div class="outline-text-4" id="text-8-7-2">
<p>
GFS applications can accommodate the relaxed consistency model with a few simple
techniques already needed for other purposes: relying on appends rather than
overwrites, checkpointing, and writing self-validating, self-identifying
records.
</p>

<p>
Practically all our applications mutate files by appending
rather than overwriting. In one typical use, a writer gener-
ates a file from beginning to end. It atomically renames the
file to a permanent name after writing all the data, or pe-
riodically checkpoints how much has been successfully writ-
ten. Checkpoints may also include application-level check-
sums. Readers verify and process only the file region up
to the last checkpoint, which is known to be in the defined
state. Regardless of consistency and concurrency issues, this
approach has served us well. Appending is far more eﬃ-
cient and more resilient to application failures than random
writes. Checkpointing allows writers to restart incremen-
tally and keeps readers from processing successfully written
file data that is still incomplete from the application’s per-
spective.
In the other typical use, many writers concurrently ap-
pend to a file for merged results or as a producer-consumer
queue. Record append’s append-at-least-once semantics pre-
serves each writer’s output. Readers deal with the occa-
sional padding and duplicates as follows. Each record pre-
pared by the writer contains extra information like check-
sums so that its validity can be verified. A reader can
identify and discard extra padding and record fragments
using the checksums. If it cannot tolerate the occasional
duplicates (e.g., if they would trigger non-idempotent op-
erations), it can filter them out using unique identifiers in
the records, which are often needed anyway to name corre-
sponding application entities such as web documents. These
functionalities for record I/O (except duplicate removal) are
in library code shared by our applications and applicable to
other file interface implementations at Google. With that,
the same sequence of records, plus rare duplicates, is always
delivered to the record reader.
</p>
</div>
</div>
</div>
</div>
</div>
</body>
</html>
